# 📅 Dev Journal – 2025-07-29

## ✅ Progress

- Refactored inference service to support **Qdrant vector database** integration
- Implemented persistent **image embedding storage**:
  - Accepts image file and metadata (`name`, `user_id`, `upload_at`)
  - Generates CLIP vector using `clip-ViT-B-32`
  - Stores embedding + metadata in Qdrant collection
- Implemented semantic **text-based image search**:
  - `/embed/text` endpoint returns top-k similar image embeddings from Qdrant
  - Used multilingual CLIP model `clip-ViT-B-32-multilingual-v1`
- Structured model loading using **`app.state`** during FastAPI startup
- Cleaned up code by moving logic into `embedded_service.py`
- Verified end-to-end flow: image upload → embedding → vector store → searchable

## 📐 Design Decisions

- Kept Qdrant access isolated in inference service to:
  - Maintain separation of concerns
  - Avoid exposing vector database directly to backend
- Used `cosine` distance as similarity metric (recommended for CLIP embeddings)
- Observed small score gaps (~0.15–0.28), still acceptable for current stage
- Disabled full-text search for now to reduce system complexity
- Will trigger embedding *after* file + metadata stored by backend (eventually async via RabbitMQ)

## 🔜 Next Up

- Integrate inference service with backend workflow:
  - Backend uploads image and metadata
  - Then sends request to inference service to process and store embedding
- Add API in backend to trigger embedding request
- Improve robustness:
  - Add batch upsert support
  - Error handling on vector storage failures
- Optionally add a `/search` endpoint to wrap text → Qdrant → metadata results

