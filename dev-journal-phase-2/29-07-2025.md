# ğŸ“… Dev Journal â€“ 2025-07-29

## âœ… Progress

- Refactored inference service to support **Qdrant vector database** integration
- Implemented persistent **image embedding storage**:
  - Accepts image file and metadata (`name`, `user_id`, `upload_at`)
  - Generates CLIP vector using `clip-ViT-B-32`
  - Stores embedding + metadata in Qdrant collection
- Implemented semantic **text-based image search**:
  - `/embed/text` endpoint returns top-k similar image embeddings from Qdrant
  - Used multilingual CLIP model `clip-ViT-B-32-multilingual-v1`
- Structured model loading using **`app.state`** during FastAPI startup
- Cleaned up code by moving logic into `embedded_service.py`
- Verified end-to-end flow: image upload â†’ embedding â†’ vector store â†’ searchable

## ğŸ“ Design Decisions

- Kept Qdrant access isolated in inference service to:
  - Maintain separation of concerns
  - Avoid exposing vector database directly to backend
- Used `cosine` distance as similarity metric (recommended for CLIP embeddings)
- Observed small score gaps (~0.15â€“0.28), still acceptable for current stage
- Disabled full-text search for now to reduce system complexity
- Will trigger embedding *after* file + metadata stored by backend (eventually async via RabbitMQ)

## ğŸ”œ Next Up

- Integrate inference service with backend workflow:
  - Backend uploads image and metadata
  - Then sends request to inference service to process and store embedding
- Add API in backend to trigger embedding request
- Improve robustness:
  - Add batch upsert support
  - Error handling on vector storage failures
- Optionally add a `/search` endpoint to wrap text â†’ Qdrant â†’ metadata results

