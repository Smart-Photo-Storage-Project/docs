# 📅 Dev Journal – 2025-07-24

## ✅ Progress

- Initialized FastAPI-based inference service for generating CLIP embeddings
- Used two models following official guidance:
  - `clip-ViT-B-32` for image embeddings
  - `clip-ViT-B-32-multilingual-v1` for text embeddings
- Implemented two endpoints:
  - `/embed/text` – accepts a string, returns embedding
  - `/embed/image` – accepts an image file, returns embedding
- Verified inference outputs via Postman with valid embeddings

## 📐 Design Decisions

- Chose to follow official SentenceTransformers recommendation to use **two aligned models** for images and text
- Separated inference logic into a **dedicated FastAPI service** to keep the backend clean and async-ready
- Decided to implement **Qdrant vector database in this phase**, enabling:
  - Persistent storage of embeddings
  - Scalable and efficient semantic search with filtering
  - Future integration with RabbitMQ for async processing
- Post-upload embedding will be handled separately to prevent blocking UX

## 🔜 Next Up

- Add Qdrant integration to inference service:
  - Store image embeddings with metadata (e.g. photo ID, user ID)
  - Prepare collection schema (vector size, distance metric)
- Backend will:
  - Send only image metadata to DB
  - Send file reference to inference service (via REST or RabbitMQ)
- Refactor flow to:
  - Decouple upload from inference
  - Enable background embedding + vector storage
- Implement search endpoint using vector similarity

