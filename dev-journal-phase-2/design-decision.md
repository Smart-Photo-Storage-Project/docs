# ğŸ“ Design Decisions â€“ Phase 2: ML Integration

This document outlines the major technical considerations and design choices made in **Phase 2** of the *Smart Photo Storage* project, focusing on **semantic search**, **image embedding**, and **machine learning integration**.

---

## ğŸ¯ Goal

To enable users to search for photos not only by filename or metadata, but also through **natural language descriptions**, such as:

- "liburan bersama keluarga di pantai"
- "sunset with friends"
- "foto saat perpisahan SMA"

This is made possible by generating **semantic embeddings** for both images and user queries, then comparing them using **cosine similarity**. By doing so, users can find relevant photos even if they don't remember the filename or the exact keywords.

---

## ğŸ§  Chosen Strategy

### Use Pretrained Multilingual Embedding Model

We use a **single off-the-shelf pretrained model**:

- **[M-CLIP (Multilingual CLIP)](https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1)** â€“ for both **image** and **text** embedding  
  > Based on OpenAIâ€™s CLIP, extended with multilingual capabilities using the same image encoder and a multilingual text encoder.

**Why this model?**
- Supports **Bahasa Indonesia** and **English**
- No need to manage two separate models
- Reduces system complexity
- Ideal for general-purpose semantic search use case
- Fine-tuning unnecessary at this stage

---

## ğŸ’¾ Embedding Storage Strategy

We decided to **introduce a dedicated vector database** in this phase.


### Why a Vector DB (Qdrant)?
- Designed specifically for fast vector similarity search
- Scales better with more images and larger embedding sets
- Native support for cosine similarity and indexing
- Enables filtering by metadata (e.g., user, tags) in future phases
- Has REST API and Python client, easily integratable with FastAPI

### Vector Storage Flow
1. Image embedding is generated by `photo-storage-inference` using M-CLIP
2. The vector (along with metadata like image ID, owner) is inserted into **Qdrant**
3. For search, query embedding is matched against vectors in Qdrant using cosine similarity
4. Qdrant returns the most relevant image IDs to the backend
5. Backend loads photo metadata and returns results to frontend

---

## ğŸŒ Language Support

We chose **M-CLIP** because it supports:

- **Bahasa Indonesia**
- **English**
- 50+ other languages

> Example: Users can search using phrases like _"foto ulang tahun"_ or _"birthday at cafe"_ interchangeably and still get relevant results.

---

## ğŸ§± System Architecture Additions (Phase 2)

| Component | Role |
|----------|------|
| `photo-storage-inference` | FastAPI service with `/embed/image` and `/embed/text` endpoints (uses M-CLIP) |
| `photo-storage-backend` | Sends photos/queries to inference service, sends vectors to Qdrant |
| `Qdrant` | Stores and indexes image vectors, performs cosine similarity search |
| `MongoDB` | Stores photo metadata (no longer stores vectors) |
| `Frontend (Vue)` | Sends natural language queries and displays semantic search results |

---

## ğŸ“Œ Summary of Phase 2 Decisions

| Area | Decision |
|------|----------|
| ğŸ”¤ Text Embedding | Use **M-CLIP** (Bahasa Indonesia + English) |
| ğŸ–¼ï¸ Image Embedding | Use **M-CLIP** image encoder (same model) |
| ğŸ” Semantic Search | Use **Qdrant** for cosine similarity |
| ğŸ’¾ Vector Storage | Store in **Qdrant** instead of MongoDB |
| âš¡ Vector DB | âœ… Implemented in this phase |
| ğŸ” Inference Strategy | Separate **FastAPI** service (Dockerized) |

---

## âœ… Next Steps (Implementation)

- [ ] Create Dockerized FastAPI inference service using M-CLIP
- [ ] Set up Qdrant with persistent storage (Docker)
- [ ] Modify backend to:
  - [ ] Send image vectors to Qdrant on upload
  - [ ] Send query vectors and retrieve similar images from Qdrant
- [ ] Update frontend to support semantic search queries

