# 📅 Dev Journal – 2025-07-30

## ✅ Progress

### 🔁 Inference Service

- [Inference] Implemented **batch image upload** endpoint (`/embed/images`):
  - Accepts multiple image files and associated metadata (`name`, `user_id`, `upload_at`, `path`)
  - Generates CLIP embeddings for each image using `clip-ViT-B-32`
  - Stores embeddings and metadata in Qdrant vector database
  - Validated request handling via `multipart/form-data`

- [Inference] Implemented **semantic search** via `/embed/text`:
  - Receives text query and returns top-k similar images from Qdrant
  - Supports filtering results by `user_id`

- [Backend] Refactored backend upload flow:
  - After saving images and metadata, backend sends batch embedding request to inference service
  - Ensures compatibility with existing photo model and storage format

- [Backend]  Integrated **semantic search** into backend:
  - Backend `/search` endpoint now delegates query to inference service
  - Reformats response into the original frontend-compatible structure (`photos`, `query`, `page`, etc.)
  - Removed MongoDB text search fallback for simplicity

- Updated `.env` and Docker network:
  - Used `host.docker.internal` for local inference connection from Dockerized backend


## 🔜 Next Up

- Containerize inference service (FastAPI + CLIP + Qdrant client)
- Add **API key authentication** to inference endpoints to restrict access to backend only
- Add validation and limits:
  - Max image count per request
  - Stricter field and format checks
- Plan async queue-based embedding trigger (via RabbitMQ)
