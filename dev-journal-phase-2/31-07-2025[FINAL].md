# 📅 Dev Journal – 2025-07-31

## ✅ Progress

- Implemented **API Key authentication** for the inference service using FastAPI's `Depends()` system  
  - API key is loaded securely via environment variable  
  - All inference endpoints now validate `x-api-key` header  
- Created and optimized **Dockerfile** for inference service
- Analyzed Docker image size (~1.7GB) and validated that `torch` is required due to `sentence-transformers`

## 📐 Design Decisions

- Standardized the use of `x-api-key` for secure access to inference endpoints  
  - Chosen for compatibility with curl, Postman, and common API gateway setups  
- Kept `torch` dependency despite large size due to its necessity for model execution  
- Deferred further Docker image optimization until post-phase cleanup (acceptable size for now)

## 🎯 Phase 2 Completion Notes

- ✅ Standalone inference service is now secure, functional, and containerized  
- ✅ Embedding API (image + text) is working and validated  
- ✅ RESTful integration between backend and inference is established  
- ✅ Qdrant setup completed for embedded image
- ✅ Backend and inference are decoupled cleanly to support future scalability

## 🔜 Next Up

- Start **Phase 3**:
  - Integrate RabbitMQ to support background embedding tasks  
  - Insert image vectors and metadata into Qdrant asynchronously  
  - Replace synchronous HTTP inference calls with event-driven workflow  
- Optimize container build and reduce dependency size (if feasible)
